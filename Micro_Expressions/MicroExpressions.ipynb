{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC1mIapOtr9D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import dlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader,Subset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJtUauHWt2O0",
        "outputId": "94a0246c-150b-4773-9cb7-09cf6fd7959e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models directory\n",
        "!mkdir -p models\n",
        "\n",
        "# Download the DLIB landmark model\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 -O models/shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# Extract the file\n",
        "!bzip2 -d models/shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "# Verify file exists\n",
        "!ls models\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2y6luSpu0Hx",
        "outputId": "63ded9ac-6fbb-4bdb-8452-f8e44e7061bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-11 21:41:42--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n",
            "--2026-01-11 21:41:42--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘models/shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "models/shape_predic 100%[===================>]  61.07M  38.9MB/s    in 1.6s    \n",
            "\n",
            "2026-01-11 21:41:44 (38.9 MB/s) - ‘models/shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n",
            "bzip2: Output file models/shape_predictor_68_face_landmarks.dat already exists.\n",
            "shape_predictor_68_face_landmarks.dat\n",
            "shape_predictor_68_face_landmarks.dat.bz2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dlib\n",
        "\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "predictor = dlib.shape_predictor(\n",
        "    \"models/shape_predictor_68_face_landmarks.dat\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZnNMKK79u5oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def landmark_heatmap_dlib(gray_img, size=48):\n",
        "\n",
        "    heatmap = np.zeros((size, size), dtype=np.float32)\n",
        "\n",
        "    img_uint8 = (gray_img * 255).astype(np.uint8)\n",
        "    faces = detector(img_uint8, 1)\n",
        "\n",
        "    if len(faces) == 0:\n",
        "        return heatmap\n",
        "\n",
        "    shape = predictor(img_uint8, faces[0])\n",
        "\n",
        "    for i in range(68):\n",
        "        x = int(shape.part(i).x * size / img_uint8.shape[1])\n",
        "        y = int(shape.part(i).y * size / img_uint8.shape[0])\n",
        "        if 0 <= x < size and 0 <= y < size:\n",
        "            heatmap[y, x] = 1.0\n",
        "\n",
        "    heatmap = cv2.GaussianBlur(heatmap, (5,5), 0)\n",
        "    return heatmap\n"
      ],
      "metadata": {
        "id": "tm1lHl2Ju7Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FER2013CSV(Dataset):\n",
        "    def __init__(self, csv_path, use_landmarks=False):\n",
        "\n",
        "        self.data = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/ResearchModule/fer2013.csv\",\n",
        "    usecols=['emotion', 'pixels', 'Usage'],\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "        self.data = self.data[self.data['Usage'] == 'Training']\n",
        "        self.pixels = self.data['pixels'].values\n",
        "        self.labels = self.data['emotion'].values\n",
        "        self.use_landmarks = use_landmarks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = np.array(\n",
        "            list(map(int, self.pixels[idx].split())),\n",
        "            dtype=np.float32\n",
        "        ).reshape(48, 48) / 255.0\n",
        "\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        if self.use_landmarks:\n",
        "            heatmap = landmark_heatmap_dlib(img)\n",
        "            img = np.stack([img, heatmap], axis=0)\n",
        "        else:\n",
        "            img = img[np.newaxis, :, :]\n",
        "\n",
        "        return torch.tensor(img, dtype=torch.float32), label"
      ],
      "metadata": {
        "id": "mGNfsf4rwwg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposedModel2(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c, k):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, k, padding=k//2, bias=False),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2),\n",
        "                nn.Dropout(0.25)\n",
        "            )\n",
        "\n",
        "        self.block1 = conv_block(1, 64, 3)\n",
        "        self.block2 = conv_block(64, 128, 5)\n",
        "        self.block3 = conv_block(128, 512, 3)\n",
        "        self.block4 = conv_block(512, 512, 3)\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(512*3*3, 256, bias=False),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(256, 512, bias=False),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "I1-gq5ELvYqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LandmarkGuidedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super().__init__()\n",
        "\n",
        "        def conv_block(in_c, out_c, k):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_c, out_c, k, padding=k//2, bias=False),\n",
        "                nn.BatchNorm2d(out_c),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2,2)\n",
        "            )\n",
        "\n",
        "        self.block1 = conv_block(2, 64, 3)   #  2 channels\n",
        "        self.block2 = conv_block(64, 128, 5)\n",
        "        self.block3 = conv_block(128, 512, 3)\n",
        "        self.block4 = conv_block(512, 512, 3)\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(512*3*3, 256, bias=False),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(256, 512, bias=False),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.fc3(x)\n"
      ],
      "metadata": {
        "id": "3H50qmjzvddq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for imgs, labels in loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "RFKKjzdMvnri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    preds, gts = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds.extend(outputs.argmax(1).cpu().numpy())\n",
        "            gts.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    return avg_loss, gts, preds\n"
      ],
      "metadata": {
        "id": "57CBhDvEGoXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_kfold_fast(model_class, dataset, max_epochs=3, max_samples=8000):\n",
        "\n",
        "    kf = KFold(n_splits=2, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for fold, (tr, val) in enumerate(kf.split(dataset)):\n",
        "        print(f\"\\nFold {fold+1}/2\")\n",
        "\n",
        "        tr = tr[:max_samples]\n",
        "        val = val[:max_samples // 2]\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(dataset, tr),\n",
        "            batch_size=64,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            Subset(dataset, val),\n",
        "            batch_size=64,\n",
        "            shuffle=False,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        model = model_class().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        #TRAINING WITH LOSS TRACK\n",
        "        for epoch in range(max_epochs):\n",
        "\n",
        "            train_loss = train_epoch(\n",
        "                model, train_loader, optimizer, criterion\n",
        "            )\n",
        "\n",
        "            val_loss, gt, pr = evaluate(\n",
        "                model, val_loader, criterion\n",
        "            )\n",
        "\n",
        "            acc = accuracy_score(gt, pr)\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch+1} | \"\n",
        "                f\"Train Loss: {train_loss:.4f} | \"\n",
        "                f\"Val Loss: {val_loss:.4f} | \"\n",
        "                f\"Val Acc: {acc:.4f}\"\n",
        "            )\n",
        "\n",
        "        p, r, f1, _ = precision_recall_fscore_support(\n",
        "            gt, pr, average='macro', zero_division=0\n",
        "        )\n",
        "\n",
        "        metrics.append((acc, p, r, f1))\n",
        "\n",
        "    return np.mean(metrics, axis=0)\n"
      ],
      "metadata": {
        "id": "ZMg5z3vmvu2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "smnQlWUmAknu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to FER-2013 CSV file (Google Drive)\n",
        "CSV_PATH = \"/content/drive/MyDrive/ResearchModule/fer2013.csv\"\n",
        "\n",
        "# Mount Google Drive to access the CSV file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Baseline dataset (NO landmarks)\n",
        "baseline_data = FER2013CSV(\n",
        "    csv_path=CSV_PATH,\n",
        "    use_landmarks=False\n",
        ")\n",
        "\n",
        "# Landmark-Guided dataset (WITH DLIB-68 heatmaps)\n",
        "lm_data = FER2013CSV(\n",
        "    csv_path=CSV_PATH,\n",
        "    use_landmarks=True\n",
        ")"
      ],
      "metadata": {
        "id": "Ce3YAI0q4ABi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c5ba37-d3b7-47db-882f-167f86941a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning Baseline CNN\")\n",
        "acc_b, p_b, r_b, f1_b = run_kfold_fast(\n",
        "    ProposedModel2,\n",
        "    baseline_data,\n",
        "    max_epochs=65\n",
        ")\n",
        "\n",
        "print(\"\\nRunning Landmark-Guided CNN\")\n",
        "acc_l, p_l, r_l, f1_l = run_kfold_fast(\n",
        "    LandmarkGuidedCNN,\n",
        "    lm_data,\n",
        "    max_epochs=65\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw5A4TWL62Qm",
        "outputId": "57dde87f-b700-426f-9d84-bc97c8921ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Baseline CNN\n",
            "\n",
            "Fold 1/2\n",
            "Epoch 1 | Train Loss: 1.9153 | Val Loss: 1.8419 | Val Acc: 0.2615\n",
            "Epoch 2 | Train Loss: 1.8179 | Val Loss: 1.7782 | Val Acc: 0.2875\n",
            "Epoch 3 | Train Loss: 1.7831 | Val Loss: 1.7699 | Val Acc: 0.3045\n",
            "Epoch 4 | Train Loss: 1.7229 | Val Loss: 1.7790 | Val Acc: 0.3135\n",
            "Epoch 5 | Train Loss: 1.6671 | Val Loss: 1.7210 | Val Acc: 0.3420\n",
            "Epoch 6 | Train Loss: 1.6163 | Val Loss: 1.6304 | Val Acc: 0.3698\n",
            "Epoch 7 | Train Loss: 1.5651 | Val Loss: 1.5767 | Val Acc: 0.3825\n",
            "Epoch 8 | Train Loss: 1.5451 | Val Loss: 1.5828 | Val Acc: 0.3802\n",
            "Epoch 9 | Train Loss: 1.5096 | Val Loss: 1.5161 | Val Acc: 0.4098\n",
            "Epoch 10 | Train Loss: 1.4718 | Val Loss: 1.5140 | Val Acc: 0.4065\n",
            "Epoch 11 | Train Loss: 1.4466 | Val Loss: 1.5220 | Val Acc: 0.4173\n",
            "Epoch 12 | Train Loss: 1.4197 | Val Loss: 1.5725 | Val Acc: 0.4110\n",
            "Epoch 13 | Train Loss: 1.3897 | Val Loss: 1.3820 | Val Acc: 0.4647\n",
            "Epoch 14 | Train Loss: 1.3607 | Val Loss: 1.3528 | Val Acc: 0.4730\n",
            "Epoch 15 | Train Loss: 1.3351 | Val Loss: 1.3740 | Val Acc: 0.4612\n",
            "Epoch 16 | Train Loss: 1.3064 | Val Loss: 1.3235 | Val Acc: 0.4838\n",
            "Epoch 17 | Train Loss: 1.2787 | Val Loss: 1.3225 | Val Acc: 0.4915\n",
            "Epoch 18 | Train Loss: 1.2637 | Val Loss: 1.3191 | Val Acc: 0.4985\n",
            "Epoch 19 | Train Loss: 1.2500 | Val Loss: 1.2855 | Val Acc: 0.5092\n",
            "Epoch 20 | Train Loss: 1.2237 | Val Loss: 1.2957 | Val Acc: 0.5040\n",
            "Epoch 21 | Train Loss: 1.1941 | Val Loss: 1.3310 | Val Acc: 0.5012\n",
            "Epoch 22 | Train Loss: 1.1753 | Val Loss: 1.2784 | Val Acc: 0.5125\n",
            "Epoch 23 | Train Loss: 1.1444 | Val Loss: 1.2986 | Val Acc: 0.5142\n",
            "Epoch 24 | Train Loss: 1.1286 | Val Loss: 1.2670 | Val Acc: 0.5182\n",
            "Epoch 25 | Train Loss: 1.1032 | Val Loss: 1.2542 | Val Acc: 0.5245\n",
            "Epoch 26 | Train Loss: 1.0786 | Val Loss: 1.2516 | Val Acc: 0.5272\n",
            "Epoch 27 | Train Loss: 1.0496 | Val Loss: 1.2910 | Val Acc: 0.5270\n",
            "Epoch 28 | Train Loss: 1.0357 | Val Loss: 1.2614 | Val Acc: 0.5310\n",
            "Epoch 29 | Train Loss: 1.0045 | Val Loss: 1.3142 | Val Acc: 0.5198\n",
            "Epoch 30 | Train Loss: 0.9837 | Val Loss: 1.2424 | Val Acc: 0.5343\n",
            "Epoch 31 | Train Loss: 0.9542 | Val Loss: 1.2472 | Val Acc: 0.5377\n",
            "Epoch 32 | Train Loss: 0.9379 | Val Loss: 1.2431 | Val Acc: 0.5395\n",
            "Epoch 33 | Train Loss: 0.9192 | Val Loss: 1.2881 | Val Acc: 0.5300\n",
            "Epoch 34 | Train Loss: 0.8898 | Val Loss: 1.3054 | Val Acc: 0.5347\n",
            "Epoch 35 | Train Loss: 0.8677 | Val Loss: 1.2594 | Val Acc: 0.5393\n",
            "Epoch 36 | Train Loss: 0.8314 | Val Loss: 1.2594 | Val Acc: 0.5490\n",
            "Epoch 37 | Train Loss: 0.8087 | Val Loss: 1.3248 | Val Acc: 0.5387\n",
            "Epoch 38 | Train Loss: 0.7935 | Val Loss: 1.2746 | Val Acc: 0.5435\n",
            "Epoch 39 | Train Loss: 0.7808 | Val Loss: 1.3012 | Val Acc: 0.5413\n",
            "Epoch 40 | Train Loss: 0.7460 | Val Loss: 1.3076 | Val Acc: 0.5415\n",
            "Epoch 41 | Train Loss: 0.7219 | Val Loss: 1.3170 | Val Acc: 0.5425\n",
            "Epoch 42 | Train Loss: 0.7052 | Val Loss: 1.3499 | Val Acc: 0.5280\n",
            "Epoch 43 | Train Loss: 0.6996 | Val Loss: 1.3529 | Val Acc: 0.5295\n",
            "Epoch 44 | Train Loss: 0.6680 | Val Loss: 1.3300 | Val Acc: 0.5425\n",
            "Epoch 45 | Train Loss: 0.6555 | Val Loss: 1.3451 | Val Acc: 0.5440\n",
            "Epoch 46 | Train Loss: 0.6304 | Val Loss: 1.3538 | Val Acc: 0.5480\n",
            "Epoch 47 | Train Loss: 0.6155 | Val Loss: 1.3931 | Val Acc: 0.5427\n",
            "Epoch 48 | Train Loss: 0.5868 | Val Loss: 1.3841 | Val Acc: 0.5485\n",
            "Epoch 49 | Train Loss: 0.5729 | Val Loss: 1.4031 | Val Acc: 0.5473\n",
            "Epoch 50 | Train Loss: 0.5573 | Val Loss: 1.4287 | Val Acc: 0.5363\n",
            "Epoch 51 | Train Loss: 0.5260 | Val Loss: 1.4122 | Val Acc: 0.5373\n",
            "Epoch 52 | Train Loss: 0.5223 | Val Loss: 1.4473 | Val Acc: 0.5500\n",
            "Epoch 53 | Train Loss: 0.5021 | Val Loss: 1.4953 | Val Acc: 0.5480\n",
            "Epoch 54 | Train Loss: 0.5039 | Val Loss: 1.4541 | Val Acc: 0.5533\n",
            "Epoch 55 | Train Loss: 0.4847 | Val Loss: 1.4492 | Val Acc: 0.5440\n",
            "Epoch 56 | Train Loss: 0.4569 | Val Loss: 1.4531 | Val Acc: 0.5445\n",
            "Epoch 57 | Train Loss: 0.4435 | Val Loss: 1.5059 | Val Acc: 0.5427\n",
            "Epoch 58 | Train Loss: 0.4356 | Val Loss: 1.5570 | Val Acc: 0.5475\n",
            "Epoch 59 | Train Loss: 0.4170 | Val Loss: 1.5532 | Val Acc: 0.5515\n",
            "Epoch 60 | Train Loss: 0.4194 | Val Loss: 1.5131 | Val Acc: 0.5483\n",
            "Epoch 61 | Train Loss: 0.4010 | Val Loss: 1.5619 | Val Acc: 0.5435\n",
            "Epoch 62 | Train Loss: 0.3860 | Val Loss: 1.5999 | Val Acc: 0.5535\n",
            "Epoch 63 | Train Loss: 0.3839 | Val Loss: 1.5933 | Val Acc: 0.5443\n",
            "Epoch 64 | Train Loss: 0.3675 | Val Loss: 1.6487 | Val Acc: 0.5275\n",
            "Epoch 65 | Train Loss: 0.3408 | Val Loss: 1.5805 | Val Acc: 0.5527\n",
            "\n",
            "Fold 2/2\n",
            "Epoch 1 | Train Loss: 1.9090 | Val Loss: 1.7932 | Val Acc: 0.2675\n",
            "Epoch 2 | Train Loss: 1.8297 | Val Loss: 1.8047 | Val Acc: 0.2752\n",
            "Epoch 3 | Train Loss: 1.7914 | Val Loss: 1.7731 | Val Acc: 0.2948\n",
            "Epoch 4 | Train Loss: 1.7323 | Val Loss: 1.7674 | Val Acc: 0.3317\n",
            "Epoch 5 | Train Loss: 1.6703 | Val Loss: 1.6832 | Val Acc: 0.3620\n",
            "Epoch 6 | Train Loss: 1.6266 | Val Loss: 1.6222 | Val Acc: 0.3820\n",
            "Epoch 7 | Train Loss: 1.5837 | Val Loss: 1.5737 | Val Acc: 0.3980\n",
            "Epoch 8 | Train Loss: 1.5532 | Val Loss: 1.5904 | Val Acc: 0.3975\n",
            "Epoch 9 | Train Loss: 1.5247 | Val Loss: 1.4744 | Val Acc: 0.4295\n",
            "Epoch 10 | Train Loss: 1.4894 | Val Loss: 1.4789 | Val Acc: 0.4285\n",
            "Epoch 11 | Train Loss: 1.4502 | Val Loss: 1.4296 | Val Acc: 0.4482\n",
            "Epoch 12 | Train Loss: 1.4269 | Val Loss: 1.4316 | Val Acc: 0.4517\n",
            "Epoch 13 | Train Loss: 1.4007 | Val Loss: 1.3873 | Val Acc: 0.4635\n",
            "Epoch 14 | Train Loss: 1.3690 | Val Loss: 1.3927 | Val Acc: 0.4620\n",
            "Epoch 15 | Train Loss: 1.3479 | Val Loss: 1.3619 | Val Acc: 0.4763\n",
            "Epoch 16 | Train Loss: 1.3206 | Val Loss: 1.3548 | Val Acc: 0.4775\n",
            "Epoch 17 | Train Loss: 1.2956 | Val Loss: 1.3238 | Val Acc: 0.4833\n",
            "Epoch 18 | Train Loss: 1.2728 | Val Loss: 1.3034 | Val Acc: 0.5150\n",
            "Epoch 19 | Train Loss: 1.2494 | Val Loss: 1.3135 | Val Acc: 0.5065\n",
            "Epoch 20 | Train Loss: 1.2187 | Val Loss: 1.2979 | Val Acc: 0.5130\n",
            "Epoch 21 | Train Loss: 1.2001 | Val Loss: 1.2830 | Val Acc: 0.5132\n",
            "Epoch 22 | Train Loss: 1.1908 | Val Loss: 1.2726 | Val Acc: 0.5228\n",
            "Epoch 23 | Train Loss: 1.1585 | Val Loss: 1.3065 | Val Acc: 0.5120\n",
            "Epoch 24 | Train Loss: 1.1386 | Val Loss: 1.2612 | Val Acc: 0.5272\n",
            "Epoch 25 | Train Loss: 1.1113 | Val Loss: 1.2559 | Val Acc: 0.5310\n",
            "Epoch 26 | Train Loss: 1.0852 | Val Loss: 1.2514 | Val Acc: 0.5282\n",
            "Epoch 27 | Train Loss: 1.0739 | Val Loss: 1.2553 | Val Acc: 0.5353\n",
            "Epoch 28 | Train Loss: 1.0400 | Val Loss: 1.2708 | Val Acc: 0.5260\n",
            "Epoch 29 | Train Loss: 1.0232 | Val Loss: 1.2520 | Val Acc: 0.5350\n",
            "Epoch 30 | Train Loss: 1.0160 | Val Loss: 1.2762 | Val Acc: 0.5327\n",
            "Epoch 31 | Train Loss: 0.9729 | Val Loss: 1.2702 | Val Acc: 0.5300\n",
            "Epoch 32 | Train Loss: 0.9539 | Val Loss: 1.2488 | Val Acc: 0.5467\n",
            "Epoch 33 | Train Loss: 0.9240 | Val Loss: 1.2543 | Val Acc: 0.5363\n",
            "Epoch 34 | Train Loss: 0.9024 | Val Loss: 1.2797 | Val Acc: 0.5292\n",
            "Epoch 35 | Train Loss: 0.8835 | Val Loss: 1.2904 | Val Acc: 0.5337\n",
            "Epoch 36 | Train Loss: 0.8467 | Val Loss: 1.3149 | Val Acc: 0.5443\n",
            "Epoch 37 | Train Loss: 0.8368 | Val Loss: 1.2989 | Val Acc: 0.5343\n",
            "Epoch 38 | Train Loss: 0.8084 | Val Loss: 1.2744 | Val Acc: 0.5415\n",
            "Epoch 39 | Train Loss: 0.7900 | Val Loss: 1.2919 | Val Acc: 0.5493\n",
            "Epoch 40 | Train Loss: 0.7780 | Val Loss: 1.4034 | Val Acc: 0.5353\n",
            "Epoch 41 | Train Loss: 0.7430 | Val Loss: 1.3934 | Val Acc: 0.5370\n",
            "Epoch 42 | Train Loss: 0.7324 | Val Loss: 1.3220 | Val Acc: 0.5417\n",
            "Epoch 43 | Train Loss: 0.7050 | Val Loss: 1.3276 | Val Acc: 0.5490\n",
            "Epoch 44 | Train Loss: 0.6886 | Val Loss: 1.4923 | Val Acc: 0.5335\n",
            "Epoch 45 | Train Loss: 0.6799 | Val Loss: 1.3948 | Val Acc: 0.5457\n",
            "Epoch 46 | Train Loss: 0.6468 | Val Loss: 1.3431 | Val Acc: 0.5533\n",
            "Epoch 47 | Train Loss: 0.6248 | Val Loss: 1.3578 | Val Acc: 0.5507\n",
            "Epoch 48 | Train Loss: 0.6019 | Val Loss: 1.4183 | Val Acc: 0.5357\n",
            "Epoch 49 | Train Loss: 0.5829 | Val Loss: 1.4411 | Val Acc: 0.5333\n",
            "Epoch 50 | Train Loss: 0.5756 | Val Loss: 1.4422 | Val Acc: 0.5510\n",
            "Epoch 51 | Train Loss: 0.5485 | Val Loss: 1.4331 | Val Acc: 0.5487\n",
            "Epoch 52 | Train Loss: 0.5394 | Val Loss: 1.4372 | Val Acc: 0.5433\n",
            "Epoch 53 | Train Loss: 0.5281 | Val Loss: 1.4371 | Val Acc: 0.5553\n",
            "Epoch 54 | Train Loss: 0.5153 | Val Loss: 1.4624 | Val Acc: 0.5450\n",
            "Epoch 55 | Train Loss: 0.5005 | Val Loss: 1.4577 | Val Acc: 0.5465\n",
            "Epoch 56 | Train Loss: 0.4825 | Val Loss: 1.5011 | Val Acc: 0.5567\n",
            "Epoch 57 | Train Loss: 0.4503 | Val Loss: 1.5001 | Val Acc: 0.5533\n",
            "Epoch 58 | Train Loss: 0.4541 | Val Loss: 1.6074 | Val Acc: 0.5367\n",
            "Epoch 59 | Train Loss: 0.4393 | Val Loss: 1.5310 | Val Acc: 0.5487\n",
            "Epoch 60 | Train Loss: 0.4269 | Val Loss: 1.5550 | Val Acc: 0.5317\n",
            "Epoch 61 | Train Loss: 0.4046 | Val Loss: 1.5951 | Val Acc: 0.5397\n",
            "Epoch 62 | Train Loss: 0.3982 | Val Loss: 1.5753 | Val Acc: 0.5473\n",
            "Epoch 63 | Train Loss: 0.3978 | Val Loss: 1.5919 | Val Acc: 0.5477\n",
            "Epoch 64 | Train Loss: 0.3690 | Val Loss: 1.5872 | Val Acc: 0.5483\n",
            "Epoch 65 | Train Loss: 0.3648 | Val Loss: 1.5794 | Val Acc: 0.5550\n",
            "\n",
            "Running Landmark-Guided CNN\n",
            "\n",
            "Fold 1/2\n",
            "Epoch 1 | Train Loss: 1.7133 | Val Loss: 1.5235 | Val Acc: 0.4085\n",
            "Epoch 2 | Train Loss: 1.4321 | Val Loss: 1.4726 | Val Acc: 0.4268\n",
            "Epoch 3 | Train Loss: 1.2681 | Val Loss: 1.4997 | Val Acc: 0.4165\n",
            "Epoch 4 | Train Loss: 1.1091 | Val Loss: 1.3588 | Val Acc: 0.4810\n",
            "Epoch 5 | Train Loss: 0.8910 | Val Loss: 1.4787 | Val Acc: 0.4630\n",
            "Epoch 6 | Train Loss: 0.6482 | Val Loss: 1.4218 | Val Acc: 0.4785\n",
            "Epoch 7 | Train Loss: 0.4280 | Val Loss: 1.5663 | Val Acc: 0.4933\n",
            "Epoch 8 | Train Loss: 0.2467 | Val Loss: 1.7697 | Val Acc: 0.4788\n",
            "Epoch 9 | Train Loss: 0.1391 | Val Loss: 1.7609 | Val Acc: 0.4768\n",
            "Epoch 10 | Train Loss: 0.0901 | Val Loss: 1.8086 | Val Acc: 0.4745\n",
            "Epoch 11 | Train Loss: 0.0554 | Val Loss: 1.8478 | Val Acc: 0.4670\n",
            "Epoch 12 | Train Loss: 0.0405 | Val Loss: 1.9217 | Val Acc: 0.4750\n",
            "Epoch 13 | Train Loss: 0.0272 | Val Loss: 1.7265 | Val Acc: 0.5040\n",
            "Epoch 14 | Train Loss: 0.0178 | Val Loss: 1.8302 | Val Acc: 0.4895\n",
            "Epoch 15 | Train Loss: 0.0150 | Val Loss: 1.7873 | Val Acc: 0.4965\n",
            "Epoch 16 | Train Loss: 0.0141 | Val Loss: 1.8467 | Val Acc: 0.4923\n",
            "Epoch 17 | Train Loss: 0.0138 | Val Loss: 1.8250 | Val Acc: 0.5102\n",
            "Epoch 18 | Train Loss: 0.0106 | Val Loss: 2.0598 | Val Acc: 0.4880\n",
            "Epoch 19 | Train Loss: 0.1133 | Val Loss: 3.3789 | Val Acc: 0.3638\n",
            "Epoch 20 | Train Loss: 0.5789 | Val Loss: 2.0048 | Val Acc: 0.4280\n",
            "Epoch 21 | Train Loss: 0.1610 | Val Loss: 1.8999 | Val Acc: 0.4808\n",
            "Epoch 22 | Train Loss: 0.0496 | Val Loss: 1.9109 | Val Acc: 0.5005\n",
            "Epoch 23 | Train Loss: 0.0266 | Val Loss: 1.8536 | Val Acc: 0.5048\n",
            "Epoch 24 | Train Loss: 0.0150 | Val Loss: 1.9001 | Val Acc: 0.5018\n",
            "Epoch 25 | Train Loss: 0.0109 | Val Loss: 1.9097 | Val Acc: 0.4985\n",
            "Epoch 26 | Train Loss: 0.0088 | Val Loss: 1.9655 | Val Acc: 0.4913\n",
            "Epoch 27 | Train Loss: 0.0081 | Val Loss: 1.9941 | Val Acc: 0.4905\n",
            "Epoch 28 | Train Loss: 0.0062 | Val Loss: 1.9970 | Val Acc: 0.4918\n",
            "Epoch 29 | Train Loss: 0.0056 | Val Loss: 2.0014 | Val Acc: 0.5082\n",
            "Epoch 30 | Train Loss: 0.0057 | Val Loss: 1.9461 | Val Acc: 0.5105\n",
            "Epoch 31 | Train Loss: 0.0040 | Val Loss: 2.0716 | Val Acc: 0.5038\n",
            "Epoch 32 | Train Loss: 0.0064 | Val Loss: 1.9942 | Val Acc: 0.5038\n",
            "Epoch 33 | Train Loss: 0.0041 | Val Loss: 2.0026 | Val Acc: 0.5015\n",
            "Epoch 34 | Train Loss: 0.0045 | Val Loss: 2.0023 | Val Acc: 0.5062\n",
            "Epoch 35 | Train Loss: 0.0037 | Val Loss: 2.0371 | Val Acc: 0.5050\n",
            "Epoch 36 | Train Loss: 0.0035 | Val Loss: 2.0225 | Val Acc: 0.5090\n",
            "Epoch 37 | Train Loss: 0.0034 | Val Loss: 2.0222 | Val Acc: 0.5045\n",
            "Epoch 38 | Train Loss: 0.0041 | Val Loss: 2.0709 | Val Acc: 0.5015\n",
            "Epoch 39 | Train Loss: 0.0042 | Val Loss: 2.0543 | Val Acc: 0.5035\n",
            "Epoch 40 | Train Loss: 0.0770 | Val Loss: 3.8720 | Val Acc: 0.3147\n",
            "Epoch 41 | Train Loss: 0.5848 | Val Loss: 2.2370 | Val Acc: 0.4308\n",
            "Epoch 42 | Train Loss: 0.1387 | Val Loss: 1.9788 | Val Acc: 0.4823\n",
            "Epoch 43 | Train Loss: 0.0402 | Val Loss: 1.9613 | Val Acc: 0.5068\n",
            "Epoch 44 | Train Loss: 0.0196 | Val Loss: 2.0222 | Val Acc: 0.4945\n",
            "Epoch 45 | Train Loss: 0.0091 | Val Loss: 1.9945 | Val Acc: 0.5108\n",
            "Epoch 46 | Train Loss: 0.0085 | Val Loss: 1.9838 | Val Acc: 0.5042\n",
            "Epoch 47 | Train Loss: 0.0082 | Val Loss: 2.0333 | Val Acc: 0.5070\n",
            "Epoch 48 | Train Loss: 0.0055 | Val Loss: 2.0251 | Val Acc: 0.5078\n",
            "Epoch 49 | Train Loss: 0.0045 | Val Loss: 2.0439 | Val Acc: 0.5072\n",
            "Epoch 50 | Train Loss: 0.0041 | Val Loss: 2.0762 | Val Acc: 0.5045\n",
            "Epoch 51 | Train Loss: 0.0040 | Val Loss: 2.0860 | Val Acc: 0.5042\n",
            "Epoch 52 | Train Loss: 0.0037 | Val Loss: 2.0727 | Val Acc: 0.5090\n",
            "Epoch 53 | Train Loss: 0.0036 | Val Loss: 2.1040 | Val Acc: 0.5020\n",
            "Epoch 54 | Train Loss: 0.0025 | Val Loss: 2.0973 | Val Acc: 0.5065\n",
            "Epoch 55 | Train Loss: 0.0037 | Val Loss: 2.1130 | Val Acc: 0.5125\n",
            "Epoch 56 | Train Loss: 0.0030 | Val Loss: 2.1105 | Val Acc: 0.5062\n",
            "Epoch 57 | Train Loss: 0.0031 | Val Loss: 2.1306 | Val Acc: 0.5082\n",
            "Epoch 58 | Train Loss: 0.0028 | Val Loss: 2.1573 | Val Acc: 0.5022\n",
            "Epoch 59 | Train Loss: 0.0026 | Val Loss: 2.1397 | Val Acc: 0.5112\n",
            "Epoch 60 | Train Loss: 0.0030 | Val Loss: 2.2022 | Val Acc: 0.5078\n",
            "Epoch 61 | Train Loss: 0.0025 | Val Loss: 2.1830 | Val Acc: 0.5122\n",
            "Epoch 62 | Train Loss: 0.0041 | Val Loss: 2.2660 | Val Acc: 0.5065\n",
            "Epoch 63 | Train Loss: 0.0033 | Val Loss: 2.1892 | Val Acc: 0.5095\n",
            "Epoch 64 | Train Loss: 0.0029 | Val Loss: 2.2615 | Val Acc: 0.5140\n",
            "Epoch 65 | Train Loss: 0.0035 | Val Loss: 2.2325 | Val Acc: 0.5162\n",
            "\n",
            "Fold 2/2\n",
            "Epoch 1 | Train Loss: 1.7211 | Val Loss: 1.5777 | Val Acc: 0.4012\n",
            "Epoch 2 | Train Loss: 1.4477 | Val Loss: 1.4588 | Val Acc: 0.4462\n",
            "Epoch 3 | Train Loss: 1.2985 | Val Loss: 1.4282 | Val Acc: 0.4602\n",
            "Epoch 4 | Train Loss: 1.1325 | Val Loss: 1.4102 | Val Acc: 0.4745\n",
            "Epoch 5 | Train Loss: 0.9484 | Val Loss: 1.4414 | Val Acc: 0.4765\n",
            "Epoch 6 | Train Loss: 0.7345 | Val Loss: 1.4649 | Val Acc: 0.4735\n",
            "Epoch 7 | Train Loss: 0.5037 | Val Loss: 1.4761 | Val Acc: 0.4953\n",
            "Epoch 8 | Train Loss: 0.3067 | Val Loss: 1.5219 | Val Acc: 0.5062\n",
            "Epoch 9 | Train Loss: 0.1814 | Val Loss: 1.8971 | Val Acc: 0.4765\n",
            "Epoch 10 | Train Loss: 0.1040 | Val Loss: 1.7025 | Val Acc: 0.5025\n",
            "Epoch 11 | Train Loss: 0.0691 | Val Loss: 1.7387 | Val Acc: 0.5012\n",
            "Epoch 12 | Train Loss: 0.0440 | Val Loss: 1.7168 | Val Acc: 0.5010\n",
            "Epoch 13 | Train Loss: 0.0378 | Val Loss: 1.8612 | Val Acc: 0.4953\n",
            "Epoch 14 | Train Loss: 0.0279 | Val Loss: 2.0244 | Val Acc: 0.4813\n",
            "Epoch 15 | Train Loss: 0.0487 | Val Loss: 2.1512 | Val Acc: 0.4720\n",
            "Epoch 16 | Train Loss: 0.1244 | Val Loss: 2.3853 | Val Acc: 0.4193\n",
            "Epoch 17 | Train Loss: 0.2695 | Val Loss: 2.2278 | Val Acc: 0.4723\n",
            "Epoch 18 | Train Loss: 0.1439 | Val Loss: 2.1196 | Val Acc: 0.4760\n",
            "Epoch 19 | Train Loss: 0.0626 | Val Loss: 1.9875 | Val Acc: 0.4968\n",
            "Epoch 20 | Train Loss: 0.0239 | Val Loss: 1.9775 | Val Acc: 0.5010\n",
            "Epoch 21 | Train Loss: 0.0166 | Val Loss: 2.0004 | Val Acc: 0.4993\n",
            "Epoch 22 | Train Loss: 0.0154 | Val Loss: 2.0714 | Val Acc: 0.5075\n",
            "Epoch 23 | Train Loss: 0.0129 | Val Loss: 2.0112 | Val Acc: 0.5142\n",
            "Epoch 24 | Train Loss: 0.0092 | Val Loss: 2.1002 | Val Acc: 0.4995\n",
            "Epoch 25 | Train Loss: 0.0067 | Val Loss: 1.9910 | Val Acc: 0.5138\n",
            "Epoch 26 | Train Loss: 0.0037 | Val Loss: 2.0179 | Val Acc: 0.5080\n",
            "Epoch 27 | Train Loss: 0.0041 | Val Loss: 1.9802 | Val Acc: 0.5200\n",
            "Epoch 28 | Train Loss: 0.0039 | Val Loss: 2.0112 | Val Acc: 0.5170\n",
            "Epoch 29 | Train Loss: 0.0037 | Val Loss: 2.0537 | Val Acc: 0.5115\n",
            "Epoch 30 | Train Loss: 0.0033 | Val Loss: 2.0184 | Val Acc: 0.5138\n",
            "Epoch 31 | Train Loss: 0.0041 | Val Loss: 2.0532 | Val Acc: 0.5148\n",
            "Epoch 32 | Train Loss: 0.0045 | Val Loss: 2.1063 | Val Acc: 0.5085\n",
            "Epoch 33 | Train Loss: 0.0048 | Val Loss: 2.1027 | Val Acc: 0.5140\n",
            "Epoch 34 | Train Loss: 0.0142 | Val Loss: 2.6027 | Val Acc: 0.4567\n",
            "Epoch 35 | Train Loss: 0.6160 | Val Loss: 1.9993 | Val Acc: 0.4828\n",
            "Epoch 36 | Train Loss: 0.1653 | Val Loss: 2.0688 | Val Acc: 0.4730\n",
            "Epoch 37 | Train Loss: 0.0444 | Val Loss: 1.9516 | Val Acc: 0.5105\n",
            "Epoch 38 | Train Loss: 0.0177 | Val Loss: 1.9780 | Val Acc: 0.5060\n",
            "Epoch 39 | Train Loss: 0.0112 | Val Loss: 1.9502 | Val Acc: 0.5140\n",
            "Epoch 40 | Train Loss: 0.0066 | Val Loss: 1.9893 | Val Acc: 0.5102\n",
            "Epoch 41 | Train Loss: 0.0060 | Val Loss: 2.0176 | Val Acc: 0.5148\n",
            "Epoch 42 | Train Loss: 0.0048 | Val Loss: 2.0351 | Val Acc: 0.5102\n",
            "Epoch 43 | Train Loss: 0.0044 | Val Loss: 2.0811 | Val Acc: 0.5120\n",
            "Epoch 44 | Train Loss: 0.0037 | Val Loss: 2.0722 | Val Acc: 0.5128\n",
            "Epoch 45 | Train Loss: 0.0038 | Val Loss: 2.0718 | Val Acc: 0.5132\n",
            "Epoch 46 | Train Loss: 0.0030 | Val Loss: 2.1126 | Val Acc: 0.5025\n",
            "Epoch 47 | Train Loss: 0.0063 | Val Loss: 2.1858 | Val Acc: 0.5108\n",
            "Epoch 48 | Train Loss: 0.0119 | Val Loss: 2.2585 | Val Acc: 0.5072\n",
            "Epoch 49 | Train Loss: 0.0069 | Val Loss: 2.2344 | Val Acc: 0.5050\n",
            "Epoch 50 | Train Loss: 0.0232 | Val Loss: 2.9635 | Val Acc: 0.4303\n",
            "Epoch 51 | Train Loss: 0.2116 | Val Loss: 2.6035 | Val Acc: 0.4520\n",
            "Epoch 52 | Train Loss: 0.1725 | Val Loss: 2.3243 | Val Acc: 0.4642\n",
            "Epoch 53 | Train Loss: 0.0610 | Val Loss: 2.2366 | Val Acc: 0.5065\n",
            "Epoch 54 | Train Loss: 0.0199 | Val Loss: 2.1905 | Val Acc: 0.5028\n",
            "Epoch 55 | Train Loss: 0.0106 | Val Loss: 2.1867 | Val Acc: 0.5080\n",
            "Epoch 56 | Train Loss: 0.0060 | Val Loss: 2.1958 | Val Acc: 0.5145\n",
            "Epoch 57 | Train Loss: 0.0045 | Val Loss: 2.1998 | Val Acc: 0.5108\n",
            "Epoch 58 | Train Loss: 0.0040 | Val Loss: 2.2311 | Val Acc: 0.5125\n",
            "Epoch 59 | Train Loss: 0.0034 | Val Loss: 2.2203 | Val Acc: 0.5028\n",
            "Epoch 60 | Train Loss: 0.0028 | Val Loss: 2.2353 | Val Acc: 0.5050\n",
            "Epoch 61 | Train Loss: 0.0027 | Val Loss: 2.2616 | Val Acc: 0.5108\n",
            "Epoch 62 | Train Loss: 0.0052 | Val Loss: 2.2830 | Val Acc: 0.5082\n",
            "Epoch 63 | Train Loss: 0.0027 | Val Loss: 2.3443 | Val Acc: 0.5125\n",
            "Epoch 64 | Train Loss: 0.0025 | Val Loss: 2.2600 | Val Acc: 0.5155\n",
            "Epoch 65 | Train Loss: 0.0026 | Val Loss: 2.3139 | Val Acc: 0.5140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n FINAL 2-FOLD RESULTS \")\n",
        "print(f\"Baseline CNN  → Accuracy:{acc_b:.4f}  Precision:{p_b:.4f}  F1-Score:{f1_b:.4f}\")\n",
        "print(f\"LM-CNN → Accuracy:{acc_l:.4f}  Precision:{p_l:.4f}  F1-Score:{f1_l:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w08dTqp3M0Yg",
        "outputId": "1c201756-504e-46c2-df69-9d1302327c70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " FINAL 2-FOLD RESULTS \n",
            "Baseline CNN  → Accuracy:0.5539  Precision:0.5211  F1-Score:0.5094\n",
            "LM-CNN → Accuracy:0.5151  Precision:0.4886  F1-Score:0.4700\n"
          ]
        }
      ]
    }
  ]
}